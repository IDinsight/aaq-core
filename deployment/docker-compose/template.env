# IMPORTANT: all variables must be either set or commented out.
# If commented out, default values will be used from `core_backend/app/config.py`

#### Root domain (if not set, localhost will be used)
# DOMAIN=example.domain.com

#### Postgres variables
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=postgres

#### Secrets - change for production
# admin user
USER1_USERNAME="admin"
USER1_PASSWORD="fullaccess"
USER1_API_KEY="admin-key"
USER1_CONTENT_QUOTA=1000

# jwt
JWT_SECRET="jwt-secret"

#### Functionality variables
# N_TOP_CONTENT_FOR_SEARCH=
# N_TOP_CONTENT_FOR_RAG=

### limits
# CHECK_CONTENT_LIMIT=True
# DEFAULT_CONTENT_QUOTA=50

# Align Score (if ALIGN_SCORE_METHOD="AlignScore", must also set the ALIGN_SCORE_API)
ALIGN_SCORE_METHOD="LLM"
ALIGN_SCORE_THRESHOLD=0.7
# ALIGN_SCORE_API=http://alignScore:5001/alignscore_base

#### Backend paths
# if using a reverse proxy, set backend root path to /api here
BACKEND_ROOT_PATH="/api"
# if using a reverse proxy, the NEXT_PUBLIC_BACKEND_URL should be "https://$DOMAIN$BACKEND_ROOT_PATH"
# if not set, it will be set to "http://localhost:8000" in the front-end
NEXT_PUBLIC_BACKEND_URL="https://localhost/api"
HUGGINGFACE_MODEL="thenlper/gte-large"
EMBEDDINGS_ENDPOINT="http://embeddings:8080"
EMBEDDINGS_API_KEY="embeddings"
# endpoint for the backend to call - must be set to the internal service name and port
LITELLM_ENDPOINT="http://litellm_proxy:4000"

# Temporary folder for prometheus gunicorn multiprocess
PROMETHEUS_MULTIPROC_DIR="/tmp"

#### Urgency detection variables
URGENCY_CLASSIFIER="llm_entailment_classifier" # or "cosine_distance_classifier"
URGENCY_DETECTION_MAX_DISTANCE=0.5 # only uses if using `cosine_distance_classifier`
URGENCY_DETECTION_MIN_PROBABILITY=0.5 # only uses if using `llm_entailment_classifier`

#### For login with Google
NEXT_PUBLIC_GOOGLE_LOGIN_CLIENT_ID="update-me"

#### Logging
## To enable langfuse logging, set LANGFUSE=True and set the keys
LANGFUSE=False
# LANGFUSE_PUBLIC_KEY="pk-..."
# LANGFUSE_SECRET_KEY="sk-..."
# optional based on your Langfuse host
# LANGFUSE_HOST="https://cloud.langfuse.com"

# Variables for LiteLLM Proxy container #######################################
# If using OpenAI APIs
OPENAI_API_KEY="sk-..."
# If using Vertex AI APIs
GOOGLE_APPLICATION_CREDENTIALS="/path/to/.gcp-credentials.json"
VERTEXAI_PROJECT="gcp-project-id-12345"
VERTEXAI_LOCATION="us-central1"
VERTEXAI_ENDPOINT="https://us-central1-aiplatform.googleapis.com"
# LiteLLM Proxy UI
UI_USERNAME="admin"
UI_PASSWORD="admin"

#### Dashboard chatbot variables

# -- IDI DB ---
DASHBOARD_AUTH_BEARER_TOKEN="aaq-db"
DASHBOARD_DB_TYPE="postgresql"
DASHBOARD_DB_SYNC_API="psycopg2"
DASHBOARD_DB_ASYNC_API="asyncpg"
DASHBOARD_DB_USER="postgres"
DASHBOARD_DB_PASSWORD='postgres' # pragma: allowlist secret
DASHBOARD_DB_HOST="relational_db"
DASHBOARD_DB_PORT="5432"
DASHBOARD_DB="postgres"
DASHBOARD_SYSTEM_MESSAGE="Ask a Question's Dashboard users will ask you questions."
DASHBOARD_DB_TABLE_DESCRIPTION="- content: Each row in this tables describes a piece of content defined by the user.\
- query: Each row is a question sent by the user to the system.\
- query-response: Each row is a successful response to a question.\
- query-response-error: A new row is added to this table for every response to a user question that was unsuccessful (an error).\
- response-feedback: Every time a user chooses to send feedback about a response, a new row is added to this table.\
- content-feedback: Every time a user chooses to send feedback about one of the contents in the search results, a new row is added to this table.\
- urgency-rule: Each row is an urgency rule, i.e. describes an urgent case, defined by the user.\
- urgency-queries: Each user query that was sent for urgency detection is stored in this table.
"
